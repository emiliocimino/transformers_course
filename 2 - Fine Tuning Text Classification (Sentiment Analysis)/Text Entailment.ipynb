{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7f0c8e-3c57-404c-ac3f-b104ec04cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "# Nel corso era in datasets load_dataset. Dalla versione di Settembre, HF ora ha spostato le metrics in evaluate\n",
    "from evaluate import load as load_metric\n",
    "import numpy as np\n",
    "\n",
    "# ATTENZIONE: per poter usare i TrainingArguments, conviene installare transformers[torch]\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
    "# Questo lo separiamo perchè è inferenza\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479d0800-e6ac-4d5a-8506-5137825e48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"rte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f75839c-d77e-48ac-b65f-adb8f191db3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 277\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e08609-84c6-4700-82bf-8c87acf64b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'not_entailment'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui Entailment =0, not entailment = 1\n",
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a1ab64-bd9e-4291-b31c-7ddcea9151ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.',\n",
       " 'sentence2': 'Pope Benedict XVI is the new leader of the Roman Catholic Church.',\n",
       " 'label': 0,\n",
       " 'idx': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b24c063-1206-4562-87fb-890049611b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(raw_datasets[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c785300-d93f-408b-bcf7-6dd67596dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.',\n",
       " 'sentence2': 'Weapons of Mass Destruction Found in Iraq.',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb68bf88-34ba-4323-82db-774fdf922337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cb5f95-25fc-4602-b57b-4c30f691ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96eaa48-6a59-4139-86ee-22957e8e0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 277/277 [00:00<00:00, 7602.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f77276b-a67d-4fdc-a6db-a09a489999db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_entailment_training\",\n",
    "    eval_strategy=\"epoch\", # Uso del dataset di evaluation per calcolare l'andamento. Default no\n",
    "    save_strategy=\"epoch\", # Salva il modello al termine di ogni epoch. Il default è per ogni step.\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=150 # evita il No Log nella prima epoch del training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9c4bdd-95c4-4a60-b94b-40487ba74d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {\"entailed\": 0, \"not_entailed\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c28f40-7021-4a50-9398-c851eb43f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2644112a-2b9c-41b5-a669-54da707e9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.id2label = {v:k for k, v in target_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b65d3d3-63ed-4e9f-af5e-e91d0e6ab0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.label2id = target_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79897614-4e8f-4f6a-b227-add6d295b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835b7e10-434f-4a3f-b5e3-de5828d9de52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              22,268,928\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 65,783,042\n",
       "Trainable params: 65,783,042\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d953547-a859-4656-a72e-b9c0b0676a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('glue', 'rte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3f29010-21e4-4a6d-964c-5213637fd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logit_and_labels):\n",
    "    logit, labels = logit_and_labels\n",
    "    predictions = np.argmax(logit, axis=-1) # axis = -1 -> Ultimo asse\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a94b38e-538b-4981-98b6-64fb4c1ac42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5290d0d-88a3-4489-b785-58b9d08ca7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 05:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.687087</td>\n",
       "      <td>0.548736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.650300</td>\n",
       "      <td>0.723308</td>\n",
       "      <td>0.545126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.839845</td>\n",
       "      <td>0.577617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>1.520054</td>\n",
       "      <td>0.599278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.831276</td>\n",
       "      <td>0.581227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=780, training_loss=0.40945693896366997, metrics={'train_runtime': 350.5, 'train_samples_per_second': 35.521, 'train_steps_per_second': 2.225, 'total_flos': 542121276647352.0, 'train_loss': 0.40945693896366997, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f88a5c9a-e341-496e-b1b5-ef1426e125c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cls = pipeline(\"text-classification\", model=\"text_entailment_training/checkpoint-156\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a6fe379-f3d1-4c28-922f-8591a4ec68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8284abec-c281-42fe-a7cd-5fb0a7d8b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': \"Mangla was summoned after Madhumita's sister Nidhi Shukla, who was the first witness in the case.\",\n",
       " 'sentence2': 'Shukla is related to Mangla.',\n",
       " 'label': -1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fd7c955-0a28-46c9-84b7-920f42654009",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [{\"text\": d[\"sentence1\"] , \"text_pair\": d[\"sentence2\"]} for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2221018e-2cd5-41d4-845e-92369bce0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_cls(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c41fae-f21d-480f-ae20-7210d2fbb392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'entailed', 'score': 0.516261875629425},\n",
       " {'label': 'entailed', 'score': 0.5346696376800537},\n",
       " {'label': 'entailed', 'score': 0.5143323540687561},\n",
       " {'label': 'entailed', 'score': 0.5035443305969238},\n",
       " {'label': 'entailed', 'score': 0.5439093112945557},\n",
       " {'label': 'not_entailed', 'score': 0.5467738509178162},\n",
       " {'label': 'entailed', 'score': 0.5089209675788879},\n",
       " {'label': 'not_entailed', 'score': 0.5052562952041626},\n",
       " {'label': 'entailed', 'score': 0.5177659392356873},\n",
       " {'label': 'entailed', 'score': 0.557256281375885}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "106f81cb-e316-4790-abfe-4547b8f6fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_idx = [target_map[d[\"label\"]] for d in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad49be9a-b79e-4c9b-a7c4-a0d40d180c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'entailed', 'score': 0.5041528344154358}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cls({\"text\": \"I just bought a car\" , \"text_pair\": \"I have a dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c260ef8-529c-4a4b-a66c-5d75f997d78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'A mercenary group faithful to the warmongering policy of former Somozist colonel Enrique Bermudez attacked an IFA truck belonging to the interior ministry at 0900 on 26 March in El Jicote, wounded and killed an interior ministry worker and wounded five others.',\n",
       " 'sentence2': 'An interior ministry worker was killed by a mercenary group.',\n",
       " 'label': -1,\n",
       " 'idx': 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597d86e-bdd2-4295-a2ee-69da4859117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc26a8-74de-433d-8198-7f28b62d7720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
