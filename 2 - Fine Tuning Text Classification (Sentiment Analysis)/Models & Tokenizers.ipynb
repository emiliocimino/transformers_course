{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9f1016a-8413-4c05-aca4-f26a09feb5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7471d-b14f-458b-931e-3c5a09aa02d9",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6224bd-69ee-4867-b68f-2ea5d54673a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. \n",
    "This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default.\n",
    "\"\"\"\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd1fde6-aec8-4bcc-8469-cb94d3c75228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7457aedb-d7e7-4368-a1ec-37b67568effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918b0b53-1381-435c-b12a-4bd96f46ef74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Hello World\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a3776c-b6d0-4f38-b949-78e48f5f21b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7592, 2088]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d16385a-374e-4474-89e4-8fceecb7e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = tokenizer.convert_ids_to_tokens(ids)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ed119b-bdc6-489e-888c-fc2c73d189aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea stringa dagli ID. Come convert ids to tokens, ma un solo output\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efae5f22-aaf1-436b-add1-3e06603ff259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 2088, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode aggiunge special tokens. Questi sono CLS (101) e SEP (102)\n",
    "ids = tokenizer.encode(\"Hello World\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7a3a15-2301-4b65-8307-a423b28eba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', 'world', '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui possiamo vedere gli special tokens\n",
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94549f21-148e-46b6-ad53-c24a761ec1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d35af55b-d608-4b33-808f-5d14f4a4535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(\"Hello World\")\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a007648-7f1b-4b81-973d-ec84da08af97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2066, 8870, 1012, 102], [101, 2079, 2017, 2066, 8870, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"I like cats.\",\n",
    "    \"Do you like cats?\"\n",
    "]\n",
    "tokenizer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c0498-ceeb-414b-b4bb-050992a23cb4",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f01872-1fb3-4811-a8df-baf7f3317858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizers e Models sono legati tra loro. Usiamo il modello associato allo stesso tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cbb12f-7651-43a0-928a-9daf21b15954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vediamo come otteniamo errore se passiamo come input le liste ottenute dal tokenizer\n",
    "# outputs = model(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bf8afbf-de00-46ad-9c64-f062d30f0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(\"Hello World\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e574830-30ba-40f3-a394-dcddc34e0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3001, -0.6187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**model_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d74b8098-d62e-4ed1-b032-12b15d6d22cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3001, -0.6187]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E' uguale a output[0]\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07856f5a-30f2-4f06-8ba4-05acfce9e58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30013636, -0.61868644]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se volessimo usare questi output per calcolare alcuni score come f1 o AUC in sklearn, abbiamo bisogno di numpy arrays\n",
    "# In ordine: detach() -> rimuove i gradienti dal tensore\n",
    "# cpu() -> porta dalla GPU alla CPU i dati\n",
    "# numpy() -> converte in numpy()\n",
    "output.logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a4428d5-1494-4ce2-a267-28b5197b8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se volessimo creare un batch, sarà necessario avere la stessa lunghezza\n",
    "# Per questo motivo dovremmo aggiungere truncate=True e padding=True per farlo funzionare o otterremo errori\n",
    "\"\"\"\n",
    "Unable to create tensor, you should probably activate truncation and/or padding \n",
    "with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
    "\"\"\"\n",
    "# In realtà padding si può anche settare con una padding strategy tra ['longest', 'max_length', 'do_not_pad']\n",
    "data = [\n",
    "    \"I like cats.\",\n",
    "    \"Do you like cats?\"\n",
    "]\n",
    "model_inputs = tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82ea362d-8128-402c-991d-a4c025a66363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0],\n",
       "        [ 101, 2079, 2017, 2066, 8870, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b851fcba-91ed-4003-8772-b4cd6704aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1823, -0.5322],\n",
       "        [ 0.1686, -0.5445]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouputs = model(**model_inputs)\n",
    "ouputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
