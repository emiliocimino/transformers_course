{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ea4ee5-c7eb-4371-9d95-1f758dc8d9de",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "Dobbiamo definire in primis ciò che è il question answering che andremo a trattare.\n",
    "Il question answering è ESTRATTIVO, ovvero che diamo un contesto e una domanda, tirando fuori dal contesto la risposta. E' diverso dal question answering generico (quello di chatGPT, insomma) che ti produce in output del testo a partire da ciò che ha appreso durante il training. Questo implica che il modello alla base non avrà un'architettura encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed7d17-cd27-4deb-acf3-0d2870db7499",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Il dataset che useremo è lo Standford Question Answering Dataset (SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c600620-7de4-4a78-a183-5f4bab618d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a228a0e-4629-4202-93b7-e116b92fcc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066deb67-be19-4ef0-9ff6-d64b85dafc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f4190066117f',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is in front of the Notre Dame Main Building?',\n",
       " 'answers': {'text': ['a copper statue of Christ'], 'answer_start': [188]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = raw_datasets[\"train\"][1]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795d533b-e618-4cbe-ad22-5a7acece6581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'University_of_Notre_Dame'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33659a2e-9dff-4f10-b23f-cd0622899a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1b919a-f517-4dd9-b7d7-f9060dc8cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a55d9e-737d-4470-9c74-10d34bc28aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in train set we don't have any chance for multiple answers or no answers\n",
    "raw_datasets[\"train\"].filter(lambda x : len(x[\"answers\"][\"text\"]) !=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624e2e82-95ae-410b-a78a-4f99dd96cd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"validation\"].filter(lambda x : len(x[\"answers\"][\"text\"]) !=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1898a8-d28b-4251-ad19-e52051608768",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "La tokenizzazione è un concetto molto interessante all'interno di questo genere di task. Come modello, useremo BERT. Siccome è QA estrattivo, non abbiamo bisogno di un'architettura encoder-decoder e ci facciamo bastare una encoder-only.\n",
    "\n",
    "Il tokenizer di BERT abbiamo visto che è in grado di prendere due input (caso text entailment). Faremo una cosa simile anche qui, inserendo prima la domanda e poi il context dove è contenuta la risposta. Tuttavia, può accadere che il context sia eccessivamente lungo, così tanto da superare il limite della finestra di contesto del tokenizer di BERT.\n",
    "\n",
    "Una soluzione valida è quella di spezzare il context in diverse window. Così facendo, passiamo al modello contemporaneamente più input (question+window) e la risposta cadrà in una delle finestre.\n",
    "\n",
    "C'è un problema: cosa succede se una risposta cade a metà tra le finestre? Inizia a diventare un po' un caos, in quanto il modello avrà pezzi di risposta in input diversi e non sarà in grado di trovarla. Per questo motivo, risolviamo \"sovrapponendo\" le finestre. E lo faremo in modo che ogni finestra sia sovrapposta a quelle precedenti esattamente a metà. Questo crea ridondanza degli input ma ci assicura di trovare risposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11170e2d-a6b4-44a3-a6b0-37220e5ed32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd580e2-d0cd-4cc0-974e-34012e6e1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604af9d4-814c-476b-be0e-bf7f49972fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed03dff-934e-4251-8afc-6e272ea21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = example[\"context\"]\n",
    "question = example[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f231a21-80c0-4aeb-8c15-e07a4f9bad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building \\' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186676a5-4200-4f4a-93be-58788f1f376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    truncation=\"only_second\", # Effettua troncamento solo sul context (second input) e non sulla question\n",
    "    max_length=100, # Lunghezza massima finestra \n",
    "    stride=50, # Quanto c'è di overlapping tra le finestre\n",
    "    return_overflowing_tokens=True, # Qui diciamo che ci interessano i token della parte  overlappata\n",
    "    return_offsets_mapping=True # Ci ritorna l'offset che esiste tra le windows e i tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a9555a-8e44-43e9-8332-54d3205398f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b26355-832f-4e8a-8fc2-382e625dc7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui è 0 perchè le finestre appartengono tutte allo stesso\n",
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac26e1a-20db-4de7-99b3-3de3cbd0e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:3][\"question\"],\n",
    "    raw_datasets[\"train\"][:3][\"context\"],\n",
    "    truncation=\"only_second\", # Effettua troncamento solo sul context (second input) e non sulla question\n",
    "    max_length=100, # Lunghezza massima finestra \n",
    "    stride=50, # Quanto c'è di overlapping tra le finestre\n",
    "    return_overflowing_tokens=True, # Qui diciamo che ci interessano i token della parte  overlappata\n",
    "    return_offsets_mapping=True # Ci ritorna l'offset che esiste tra le windows e i tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a1f425-617f-46b1-ad50-19615666c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd5a9498-a39f-49b6-b090-525a781151aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building ' s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    truncation=\"only_second\", # Effettua troncamento solo sul context (second input) e non sulla question\n",
    "    max_length=100, # Lunghezza massima finestra \n",
    "    stride=50, # Quanto c'è di overlapping tra le finestre\n",
    "    return_overflowing_tokens=True, # Qui diciamo che ci interessano i token della parte  overlappata\n",
    "    return_offsets_mapping=True # Ci ritorna l'offset che esiste tra le windows e i tokens\n",
    ")\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7473c86e-7e59-493e-83eb-0396a4279e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo notare una cosa: questo output è una lista di liste di tuple\n",
    "# Ad ogni indice tutto inzia sempre con la domanda, ma poi quando inizia il context il conteggio della posizione si riporta\n",
    "# esattamente alla posizione contenuta nella frase originale, non alla posizione dell'input.\n",
    "# P.s: i token speciali non hanno uno spazio, motivo per il quale si indicano con (0,0)\n",
    "print(inputs[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fab21-24f8-44ae-a7e2-4cb61f356b72",
   "metadata": {},
   "source": [
    "### Allineamento dei target\n",
    "\n",
    "Dobbiamo allineare i target, ora. Questo perchè la posizione dell'answer varia da window a window, adesso. Dobbiamo prendere la posizione assoluta dell'answer e sottrarre l'offset della window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b6b78bb-b9cd-44f1-a49c-a072d00c987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c8a95ef-6ce8-4777-93df-460735e883c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None] 100\n"
     ]
    }
   ],
   "source": [
    "# Questa funzione (sequence_ids) prende in input un indice (indice della lista esterna) e restituisce gli id.\n",
    "# Dove troviamo 0, c'è la question. Dove c'è 1, sta la window di contesto. Altrimenti sono token speciali.\n",
    "# Se usassimo BERT, potremmo avere questi campi direttamente nel \"token_type_id\"\n",
    "print(inputs.sequence_ids(0), len(inputs.sequence_ids(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1094ab2-11dc-47b5-813f-084444109c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quel è il problema? Che dividendo la finestra di contesto, la posizione della risposta parte a 181.\n",
    "# Questo significa che la risposta è presente sia nella finestra 100-200 che in quella 150 2500.\n",
    "# nel primo caso la lunghezza relativa è 81, nel secondo 31\n",
    "answer = raw_datasets[\"train\"][1][\"answers\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f542704a-6ee5-44c0-a3bf-d9e6731da2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Il tipo è una lista\n",
    "type(inputs.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91cc93e2-a411-490b-bde1-c5d3eb917213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prima occorrenza di 1\n",
    "sequence_ids.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bef00959-ea9c-432d-b613-28afd588ad5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 98)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invertiamo i sequence ids e vediamo dove si trova l'1 ora:\n",
    "# Siccome l'ultimo token è il SEP, avremo ovviamente che la stringa al contrario è il primo.\n",
    "# Usiamo questa informazione per tagliare, dalla lunghezza massima, il valore di indice\n",
    "sequence_ids[::-1].index(1), len(sequence_ids) - sequence_ids[::-1].index(1) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b70a7bb8-2d85-4e53-a576-bca5350ab4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13], dtype=int64), array([98], dtype=int64))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in alternativa...\n",
    "np.argwhere(sequence_ids)[0], np.argwhere(sequence_ids)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a950c76-e38d-4ec6-9daf-1ef649b2ea43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 98)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ora vogliamo usare questa funzione in combinazione con gli ids per trovare il context:\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "ctx_start = sequence_ids.index(1) # -> Ci dice l'indice della prima occorrenza dell'1\n",
    "ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) -1\n",
    "\n",
    "ctx_start, ctx_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce0be9fe-ee5f-4f2f-b4df-c52105cc5cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 57)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ora arriviamo a trovare la posizione della risposta nelle finestre di contesto, a partire dall'indice della risposta nella\n",
    "# lista. quindi mapperemo la risposta da \"char index\" a \"token index\" e poi da \"token index (full context) a \"window token idx\".\n",
    "# Se non è presente nella finestra, usremo (0,0) come coordinata del token [spazio nullo]\n",
    "# Faremo questa cosa staticamente per solo la prima finestra. Poi inseriremo tutto in un loop\n",
    "ans_start_char = answer[\"answer_start\"][0] #188\n",
    "ans_end_char = ans_start_char + len(answer[\"text\"][0]) #188+ len\n",
    "\n",
    "offset = inputs[\"offset_mapping\"][0]\n",
    "start_idx, end_idx = 0, 0\n",
    "# RICORDA: Offset = tupla (x, y) dove il primo valore (x) è l'indice di partenza del token, y è l'indice di arrivo\n",
    "# Se il primo valore di offset della prima parola è maggiore dell'indice di inizio della risposta \n",
    "# OPPURE se l'ultimo valore di offset dell'ultima parola è minore dell'indice di fine della risposta\n",
    "if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "    print(\"(0, 0)\") # Fai nulla\n",
    "else:\n",
    "    # Troviamo la posizione relativa della rispsota nella finestra di contesto.\n",
    "    # Loopiamo negli offset, cerchiamo linearmente i token all'interno della tupla\n",
    "    i = ctx_start # Partiamo da qui per evitare di trovare la posizione nella domanda.\n",
    "    for start_end_char in offset[ctx_start:]:\n",
    "        start, end = start_end_char\n",
    "        if start == ans_start_char:\n",
    "            start_idx = i\n",
    "        if end == ans_end_char:\n",
    "            end_idx = i\n",
    "            break # il break solo qui\n",
    "        i+=1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ea1b8d4-4c8e-439f-b330-4ca10f9b26c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([170, 7335, 5921, 1104, 4028], 'a copper statue of Christ')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vediamo se è corretto:\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "input_ids[start_idx:end_idx+1] , tokenizer.decode(input_ids[start_idx:end_idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed8adcbe-0b8f-485f-9905-cff1906e2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scriviamoci la funzione\n",
    "def find_answer_token_idx(ctx_start, ctx_end, ans_start_char, ans_end_char, offset):\n",
    "    start_idx, end_idx = 0, 0\n",
    "    #Se la risposta non è interamente contenuta nel context:\n",
    "    if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "        pass\n",
    "        # Ritorna 0, 0 come settato all'inizio\n",
    "    else:\n",
    "        i = ctx_start\n",
    "        for start_end_char in offset[ctx_start:]:\n",
    "            start, end = start_end_char\n",
    "            if start == ans_start_char:\n",
    "                start_idx = i\n",
    "            if end == ans_end_char:\n",
    "                end_idx = i\n",
    "                break # il break solo qui\n",
    "            i+=1\n",
    "    \n",
    "    return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd25db98-38e5-4fb5-be24-7983a1c98fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([53, 17, 0, 0], [57, 21, 0, 0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mettiamo tutto in un loop per tutte le finestre di contesto, usando questa funzione!\n",
    "\n",
    "start_idxs, end_idxs = [], []\n",
    "for i, offset in enumerate(inputs['offset_mapping']):\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    context_ids = np.argwhere(sequence_ids)\n",
    "    ctx_start, ctx_end = context_ids[0][0], int(context_ids[-1][0])\n",
    "\n",
    "    start_idx, end_idx = find_answer_token_idx(ctx_start, ctx_end, ans_start_char, ans_end_char, offset)\n",
    "    start_idxs.append(start_idx)\n",
    "    end_idxs.append(end_idx)\n",
    "\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a8fea-eb5b-4538-b23d-b0d67ef416d7",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "A differenza del solito, questa volta i tokenizer saranno due. Uno in fase di training, uno in fase di evaluating.\n",
    "Key points:\n",
    "1. Dobbiamo creare le context window\n",
    "2. Siccome molti dati raggiungeranno la max length, il padding lo inseriamo nel tokenizer e sarà uguale per tutti\n",
    "3. Dobbiamo allineare le answer alle context window\n",
    "\n",
    "Il motivo dei due tokenizer è perchè facciamo operazioni diverse sugli output. Nel tokenizer di validazione, infatti, le metriche lavoreranno direttamente sulle stringhe, non sugli IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e828488-57f2-4786-bf57-926c52d31e3b",
   "metadata": {},
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c33365d-8fd8-4acd-9854-9639a30800a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what city and state did Beyonce  grow up? \n",
      " The album, Dangerously in Love  achieved what spot on the Billboard Top 100 chart?\n",
      "Which song did Beyonce sing at the first couple's inaugural ball? \n",
      "What event did Beyoncé perform at one month after Obama's inauguration? \n",
      "Where was the album released? \n",
      "What movie influenced Beyonce towards empowerment themes? \n"
     ]
    }
   ],
   "source": [
    "# In primo luogo, alcune domande sono mal formattate ed hanno degli spazi prima e dopo. Vediamo quali:\n",
    "for q in raw_datasets[\"train\"][\"question\"][:1000]:\n",
    "    if q.strip() != q:\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcaee061-db5f-48f7-87bf-928e17eb61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Questi valori vengono da esperimenti di google:\n",
    "max_lenght = 384\n",
    "stride = 128\n",
    "\n",
    "def tokenizer_fn_train(batch):\n",
    "\n",
    "    questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        batch[\"context\"],\n",
    "        max_length=max_lenght,\n",
    "        stride=stride,\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Siccome non avremo bisogno delle colonne dopo, le rimuoviamo per non ritrovarcele in uscita\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = batch[\"answers\"]\n",
    "\n",
    "    start_idxs, end_idxs = [], []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        # Qui noi stiamo semplicemente recuperando la risposta dal dataset originale (raw) perchè il tokenizer crea quello espanso\n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        answer = answers[sample_idx]\n",
    "        \n",
    "        ans_start_char = answer[\"answer_start\"][0]\n",
    "        ans_end_char = ans_start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        context_ids = np.argwhere(sequence_ids)\n",
    "        ctx_start, ctx_end = context_ids[0][0], int(context_ids[-1][0])\n",
    "\n",
    "        start_idx, end_idx = find_answer_token_idx(ctx_start, ctx_end, ans_start_char, ans_end_char, offset)\n",
    "        start_idxs.append(start_idx)\n",
    "        end_idxs.append(end_idx)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_idxs\n",
    "    inputs[\"end_positions\"] = end_idxs\n",
    "    return inputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43509283-cbf5-43fc-b8e4-0e1c04d23380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 87599/87599 [00:24<00:00, 3566.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(tokenizer_fn_train, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a3e671fb-99dc-4b7b-862e-e4dacfb9fd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f9373-1890-414a-ae77-eecc862afc09",
   "metadata": {},
   "source": [
    "### Eval Tokenizer\n",
    "\n",
    "Il tokenizer della validazione non necessita dei target. La risposta sarò comparata direttamente sulle stringhe\n",
    "\n",
    "Una seconda differenza riguarda l'offset mapping: metteremo a \"None\" i token relativi alla domanda, scopriremo perchè dopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df66daf1-e391-46d5-9a78-239c12402915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Terremo gli \"id\", gli identificatori univoci del campione nel dato. Serviranno per matchare correttamente in fase di eval\n",
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d5e05cd8-9673-46b4-9e21-aebccee3a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn_validation(batch):\n",
    "\n",
    "    questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        batch[\"context\"],\n",
    "        max_length=max_lenght,\n",
    "        stride=stride,\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Siccome non avremo bisogno delle colonne dopo, le rimuoviamo per non ritrovarcele in uscita\n",
    "    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    sample_ids = [] # Qui ci saranno gli id alfanumerici\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = orig_sample_idxs[i]\n",
    "        sample_ids.append(batch[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "\n",
    "        # Lascia l'offset invariato se è relativo al contesto, altrimenti diventa un None\n",
    "        inputs[\"offset_mapping\"][i] = [x if sequence_ids[j] == 1 else None for j, x  in enumerate(offset)]\n",
    "\n",
    "    inputs[\"sample_id\"] = sample_ids\n",
    "\n",
    "    return inputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3cd645db-0444-4179-8526-6acd2a1f15c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 10570/10570 [00:03<00:00, 2931.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    tokenizer_fn_validation, batched=True, remove_columns=raw_datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "694ee654-f115-47af-ae14-b4d4543708d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9ec61-fe8c-4de6-ab4e-78d42a74f628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
