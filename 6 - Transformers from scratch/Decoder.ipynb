{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aecb56",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03cb659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2aef00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # creiamo la causal mask, motivo per il quale passiamo anche la max_len nell'init\n",
    "        # la causal mask ci permette di diagonalizzare la matrice dei pesi dell'attenzione\n",
    "        # torch.tril è triangular lower (gli 1 stanno sotto)\n",
    "        cm = torch.tril(torch.ones(max_len, max_len))\n",
    "        # registriamo questa matrice con questa forma: 1 x 1 x max_len x max_len\n",
    "        self.register_buffer(\"causal_mask\", cm.view(1, 1, max_len, max_len))\n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "        \n",
    "        N = q.shape[0]\n",
    "        T = q.shape[1]\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if pad_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(pad_mask[:, None, None, :] == 0, float(\"-inf\"))\n",
    "        # Causal Mask si applica su tutte le dimensioni, fino alla lunghezza dell'input per velocizzare\n",
    "        attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T, self.d_k * self.n_heads)\n",
    "        \n",
    "        return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ac137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = CausalSelfAttention(d_k, d_model, n_heads, max_len)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "       \n",
    "    \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c326fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        div_term = torch.exp(exp_term * (- math.log(10000) / d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0,:,0::2] = torch.sin(position * div_term) \n",
    "        pe[0,:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :] \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6706a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [TransformerBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size) # Qui cambia rispetto all'encoder! Abbiamo bisogno della vocab size\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "            \n",
    "        # Qui c'è la differenza: prima noi prendevamo solo il primo hidden vector (x[:,0,:]) per il calcolo dei logits\n",
    "        # Ora invece noi calcoliamo simultaneamente T risultati\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x) # many to many\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a375140",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab9bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(20000, 1024, 16, 64, 4, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ef66e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=20000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58fd180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 20000, size=(8,512))\n",
    "x_t = torch.tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafeb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((8, 512))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ddd0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 20000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(x_t, mask_t)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c1b25",
   "metadata": {},
   "source": [
    "## Tokenizer & Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e265864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef86e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db807057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siccome abbiamo bisogno solo di testi, usiamo lo stesso dataset del GLUE, ma eliminiamo le labels\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9533d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb84fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True).remove_columns([\"sentence\", \"idx\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "926d64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58de06c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cefc8e",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4259a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d45276",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e46093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids v: tensor([[  101,  1767,   170,  ...,     0,     0,     0],\n",
      "        [  101,   112,   188,  ...,     0,     0,     0],\n",
      "        [  101,  1142,  1110,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  9279, 14174,  ...,     0,     0,     0],\n",
      "        [  101,  1115,  1132,  ...,     0,     0,     0],\n",
      "        [  101,  1106,  1920,  ...,     0,     0,     0]])\n",
      "k: attention_mask v: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(\"k:\", k, \"v:\", v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b11bbac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e0607",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ac0a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(28996, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): CausalSelfAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=tokenizer.model_max_length,\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332dd8e0",
   "metadata": {},
   "source": [
    "### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a33c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nel criterio, ignoriamo tutti i pad token, dato che stiamo facendo predizione \"many to many\"\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cf987",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb0f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66fb88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = [] # Questa volta la train loss sarà mediata per ogni campione\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # shiftiamo i target indietro, cloniamo e stacchiamo i gradienti dal clone\n",
    "            targets = batch[\"input_ids\"].clone().detach()\n",
    "            targets = torch.roll(targets, shifts=-1, dims=1) # rolliamo sulla dimensione T\n",
    "            targets[:, -1] = tokenizer.pad_token_id # inseriamo un padding all'ultimo, dato lo shift\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            # V è la vocabulary size\n",
    "            # Gli output avranno una dimensione pari a N x T x V, ma pytorch si aspetta N x V x T nel criterion\n",
    "            # Per questo motivo faremo una trasposizione degli output in questo modo:\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item()) # loss nel batch\n",
    "        \n",
    "        # semplificazione: facciamo la media nel batch, anche se il numero di esempi per batch (T) sarà diverso\n",
    "        train_loss = np.mean(train_loss) \n",
    "        train_losses[it] = train_loss\n",
    "        \n",
    "        dt = datetime.now() - t0\n",
    "        print(f\"Epoch {it+1}/{epochs} - Train Loss: {train_loss:.4f}, Duration: {dt}\")\n",
    "    \n",
    "    return train_losses\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3367702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 5.9765, Duration: 0:02:25.279204\n",
      "Epoch 2/15 - Train Loss: 5.0193, Duration: 0:02:28.784875\n",
      "Epoch 3/15 - Train Loss: 4.6853, Duration: 0:02:26.298491\n",
      "Epoch 4/15 - Train Loss: 4.4975, Duration: 0:02:24.495420\n",
      "Epoch 5/15 - Train Loss: 4.3637, Duration: 0:02:23.794512\n",
      "Epoch 6/15 - Train Loss: 4.2583, Duration: 0:02:23.610118\n",
      "Epoch 7/15 - Train Loss: 4.1687, Duration: 0:02:22.736529\n",
      "Epoch 8/15 - Train Loss: 4.0917, Duration: 0:02:23.520232\n",
      "Epoch 9/15 - Train Loss: 4.0235, Duration: 0:02:23.355400\n",
      "Epoch 10/15 - Train Loss: 3.9621, Duration: 0:02:23.323222\n",
      "Epoch 11/15 - Train Loss: 3.9050, Duration: 0:02:23.572858\n",
      "Epoch 12/15 - Train Loss: 3.8534, Duration: 0:02:23.397059\n",
      "Epoch 13/15 - Train Loss: 3.8059, Duration: 0:02:22.258610\n",
      "Epoch 14/15 - Train Loss: 3.7609, Duration: 0:02:22.600506\n",
      "Epoch 15/15 - Train Loss: 3.7202, Duration: 0:02:23.992705\n"
     ]
    }
   ],
   "source": [
    "train_losses = train(model, criterion, optimizer, train_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8ec61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "479ad700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in valid_loader:\n",
    "    batch = {k:v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1245d45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 28996])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c282821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 170,  112,  188,  170, 2523,  117, 2232, 6276, 7494,  102,  102, 1104]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44a3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ids = torch.argmax(outputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97a21e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] it ' s a charming and often affecting journey. [SEP]\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "191697da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a ' s a movie, moving funny portrait [SEP] [SEP] of\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffc4e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] it ' s a movie\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stiamo concatenando i primi cinque token dell'input con il risultato della predizione\n",
    "# NB: siccome l'input include 0-1-2-3-4, noi attualmente vorremmo l'output del 4, che è esattamente prediction_ids[0,4]\n",
    "# se usiamo lo 0, però, da problemi.\n",
    "tokenizer.decode(torch.concat((batch[\"input_ids\"][0, :5], prediction_ids[:, 4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb390c5",
   "metadata": {},
   "source": [
    "### Inferenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94c9e6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1122,  112,  188,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generiamo qualcosa\n",
    "prompt = \"it's\"\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f573e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 28996])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gli leviamo il [SEP] token manualmente, che definisce la fine della frase\n",
    "ouputs = model(\n",
    "    tokenized_prompt[\"input_ids\"][:, :-1].to(device),\n",
    "    tokenized_prompt[\"attention_mask\"][:, :-1].to(device)\n",
    ")\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09f7cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendiamo unicamente l'ultimo timestep, cercando il massimo sulla dimensione del vocab\n",
    "prediction_ids = torch.argmax(outputs[:,-1,:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835c6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a786b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Loop\n",
    "prompt = \"I think there\"\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tokenized_prompt[\"input_ids\"][:, :-1].to(device)\n",
    "mask = tokenized_prompt[\"attention_mask\"][:, :-1].to(device)\n",
    "\n",
    "for _ in range(100):\n",
    "    outputs = model(input_ids, mask)\n",
    "    prediction_id = torch.argmax(outputs[:, -1, :], axis=-1)\n",
    "    \n",
    "    # appendiamo il nuovo risultato (come token id). Hstack è horizontal stack (columnwise stack)\n",
    "    input_ids = torch.hstack((input_ids, prediction_id.view(1, 1)))\n",
    "    mask = torch.ones_like(input_ids) # genero la maschera nuova sul nuovo input, basandomi sulla shape\n",
    "    \n",
    "    if prediction_id == tokenizer.sep_token_id:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14ac2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] I think there ' s no reason to see this movie, but it ' s a movie that ' s not a participant. [SEP]\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28279da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
