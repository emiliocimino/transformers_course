{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb60252a-2b5d-4cae-b39c-1618bf7cd13d",
   "metadata": {},
   "source": [
    "# Encoder Architecture from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd42fc6-95ef-4c04-815d-c01f6469ff13",
   "metadata": {},
   "source": [
    "## Class Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c96b2e-cc06-4e66-8df6-09c74b63f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3754c7a-9076-4529-ba09-5dbac3c5791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # assumiamo d_v = d_k = d_q\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Siccome python fa schifo con i for loops, creiamo automaticamente delle matrici con le heads concatenate\n",
    "        # Stiamo cercando di implementare le operazioni matriciali utilizzando dei layer Linear. Questi tuttavia aggiungono un vettore bias\n",
    "        # E' possibile settare a 0 il bias, volendo, ma non lo faremo\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "        # Layer finale fully connected\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, k, q, v, mask=None):\n",
    "        \"\"\"\n",
    "        In input, qui, abbiamo i valori di key, query e value che abbiamo in input.\n",
    "        La formula, infatti, richiede dei vettori di key, query e value che, moltiplicati per alcune matrici di pesi, restituiscono\n",
    "        dei vettori con un \"significato\". Qui, in poche parole, stiamo creando le quey, le key e i valori che comprendono il testo\n",
    "        \"\"\"\n",
    "        # Teniamo traccia delle dimensioni: l'input è formato da N esempi di lunghezza T di vettori di dimensione d_model\n",
    "        q = self.query(q) # N x T x (hd_k) -> h numero di attention heads\n",
    "        k = self.key(k) # N x T x (hd_k)\n",
    "        v = self.value(v) # N x T x (hd_v)\n",
    "\n",
    "        N = q.shape[0]\n",
    "        T = q.shape[1]\n",
    "\n",
    "        \"\"\"\n",
    "        Come se non bastasse, nonostante abbiamo un tensore tridimensionale, per far funzionare le cose dobbiamo fare uno split ulteriore\n",
    "        Vogliamo portare la dimensione a N x T x h x d_k, per poi invertire la dimensione 1 con la dimensione 2\n",
    "        Per far questo, usiamo due funzioni di torch: view, che è l'equivalente di reshape, e transpose. Una trasposizione a più dimensioni\n",
    "        ti permette di invertire due dimensioni passandoci in input gli assi\n",
    "        \"\"\"\n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Facciamo il primo passaggio: Q*K.T / sqrt(d_k)\n",
    "        (N x h x T x d_k) * (N x h x d_k x T) = (N x h x T x T) -> vogliamo una matrice T x T per gli attention head, per ogni head, per ogni batch\n",
    "        la @ in torch indica un prodotto scalare, utile per il broadcasting in questo caso. Il broadcast implementa implicitamente un for loop\n",
    "        In poche parole le prime due dimensioni vengono calcolate \"element-wise\" mentre le ultime due con un dot product. come se facessimo\n",
    "        for n in range(N): for h in range(H): score[n, h] = q[n, h] dot k[n, h].transpose\n",
    "        \"\"\"\n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        # Applichiamo masking\n",
    "        # Il masking serve per riconoscere quali dei token è token e quale è padding ed ha dimensione (N x T), quindi bidimensionale\n",
    "        # Noi dobbiamo applicarla ad un tensore quadridimensionale. Usando mask [:, None, None, :] stiamo aggiungendo dimensioni extra, tanto da\n",
    "        # avere come risultato qualcosa di simile a (N x 1 x 1 x T)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                mask[:, None, None, :] == 0, float(\"-inf\")\n",
    "            )\n",
    "        # applichiamo softmax all'ultima dimensione\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # ora moltiplichiamo i pesi dell'attenzione per il valore\n",
    "        # (N x h x T x T) x (N x h x T x d_k) = (N x h x T x d_k)\n",
    "        A = attn_weights @ v\n",
    "\n",
    "        # facciamo tornare tutto alla dimensione iniziale prima del layer finale\n",
    "        # L'uso di contiguous serve a essere sicuri che la matrice sia correttamente piazzata in memoria (contiguità)\n",
    "        A = A.transpose(1, 2) # (N x T x h x d_k)\n",
    "        A = A.contiguous().view(N, T, self.n_heads * self.d_k)\n",
    "\n",
    "        # ritorniamo la proiezione lineare\n",
    "        return self.fc(A)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03c6eec-9607-4430-a109-d2a4c8283dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MutiHeadAttention(d_k, d_model, n_heads)\n",
    "        # Ricorda: in torch un layer si istanzia esplicitando input e output, ad eccezione della batch ovviamente)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input (N x T x d_model), mask = (N x T)\n",
    "        x = self.ln1(x + self.mha(x, x, x, mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbbda4-d0f4-4ff9-97bf-527582885234",
   "metadata": {},
   "source": [
    "#### La formula in breve:\n",
    "La formula del positional encoding è la seguente:\n",
    "\n",
    "$PE(pos, 2i) = sin(\\frac{pos}{10000^{\\frac{2i}{dmodel}}})$\n",
    "\n",
    "$PE(pos, 2i+1) = cos(\\frac{pos}{10000^{\\frac{2i}{dmodel}}})$\n",
    "\n",
    "Esiste un modo semplice per riscrivere il tutto nell'operazione giù per evitare caos con la stabilità numerica, dato che 10000 è un numero molto grande mentre e è molto piccolo.\n",
    "\n",
    "Se prendiamo $10000^{-\\frac{2i}{dmodel}}$ possiamo trasformare il tutto con: $(e^{-log(10000)})^{\\frac{2i}{dmodel}}$ = $(e^{\\frac{-log(10000)*2i}{dmodel}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82482fd7-c9e2-4e71-89d4-5e4880ec5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ricordiamo le formule per il positional encoding:\n",
    "PE(pos, 2i) = sin(pos/10000^[2i/dmodel])\n",
    "PE(pos, 2i+1) = cos(pos/10000^[2i/dmodel])\n",
    "\"\"\"\n",
    "# Max_len serve perchè dobremmo sapere quanti output dobbiamo precalcolare\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # creiamo un tensore di interi tra 0 e mex_len, ci aggiungiamo una dimensione. L'uso di \"1\" ci fa creare un tensore colonna\n",
    "        # indicando l'asse che vogliamo espandere. Otteniamo un array (max_len x 1)\n",
    "        position = torch.arange(max_len).unsqueeze(1) # Pos in formula\n",
    "        exp_term = torch.arange(0, d_model, 2) # Array di esponenti, da 0 a d_model contando a due a due. Sono gli esponenti di 10k\n",
    "        div_term = torch.exp(exp_term * (- math.log(10000) / d_model)) # VEDERE SOPRA\n",
    "\n",
    "        # inzializziamo pe di dimensione (1 x max_len x d_model). La prima dimensione serve per rendere tutto broadcastabile\n",
    "        # in quanto pe dev'essere aggiunto all'input che ha dimensione N x T x d_model\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        # assegnamo i valori corretti di pe. Nota che usiamo pos * div_term perchè div term ha l'esponenente negativo\n",
    "        pe[0,:,0::2] = torch.sin(position * div_term) # ultima dimensione da 0 ogni 2 (pari)\n",
    "        pe[0,:,1::2] = torch.cos(position * div_term) # ultima dimensione: da 1 ogni 2 (dispari)\n",
    "\n",
    "        # Questa istruzione ci permette di salvare e caricare correttamente il modello\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input di shape (N x T x d_model)\n",
    "        # Ricordiamo: pytorch ci permette di mettere in input sequenze di dimensioni differenti, quindi T variabile\n",
    "        x = x + self.pe[:, :x.size(1), :] # Qui stiamo dicendo di elaborare fino alla lunghezza massima sulla seconda dim.\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c92eca-8f29-4375-b22f-dfeb2ff64f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, n_classes, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [TransformerBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformers_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        # facciamo il for, in quanto è necessario passare la mask per ogni transformer block\n",
    "        for block in self.transformers_blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Qui facciamo una cosa: siccome stiamo codificando un'informazione, prendiamo uno degli hidden vectors a caso (il primo per convenzione)\n",
    "        # Shape partenza: (N x T x d_model) -> (N x d_model)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c242-73db-43c5-8045-dea9ee4c83e9",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0bbd38f-a498-40a7-b4ce-065eda74e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testiamo il funzionamento dell'encoder!\n",
    "model = Encoder(20000, 1024, 16, 64, 4, 2, 5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b471580-5cde-4e99-977d-0eba8fd736ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformers_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MutiHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MutiHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef468257-cba9-4d4e-b6f5-35cdaf29bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un vettore di dimensione 8 x 512 con valori interi che vanno da 0 alla vocab_size dummy inserita\n",
    "# La dimensione specificata è NxT\n",
    "x = np.random.randint(0, 20000, size=(8,512))\n",
    "x_t = torch.tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe80ee9-f154-4980-acb3-7eb0269d22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo una mask corrispondente di dimensione\n",
    "mask = np.ones((8, 512))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0257210-98a9-47c9-9908-e2ca600bc604",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x_t, mask_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cf05d8-ac06-4f94-a5c2-897434cc70a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# la shape ha senso. Sono 8 samples con le 5 classi corrispondenti\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2166910-eb25-4864-8570-4a4b417c8f1c",
   "metadata": {},
   "source": [
    "## Tokenizer & Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ecdb61b-b650-434c-8618-8e42d1d00509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8f554d-18b1-4779-b327-d4b71eb2f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f33fe0-0b27-40e1-ab3f-de8a13a74563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teniamo le cose semplici ed usiamo il dataset della sentiment analysis per il GLUE benchmark\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cabec67-abd1-47e2-abd8-fea0cf7c74e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f5c645-da7b-4387-b1d0-b5d7adb83597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc0c074-21c0-4068-98fd-020dd0646c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 28504.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6e7664-aa75-4cdc-88bc-7b02180ad4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45db4d87-992f-46ee-85b6-0033e51ba320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12229b2e-99ce-4850-807e-69bd40ae7f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3556a8c-2f6d-441b-959f-146f54768b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f640d4-1966-40a2-8acf-8063d7335d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79649a13-b4d6-45ef-ad4c-07fd2743c348",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3934cb2-66ef-4d7b-90c8-8950e81736f2",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f3524d9-1eff-4b74-9a09-67690eb7370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b80e96e-a21a-421b-a385-e29f7ab2decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator # Inseriamo il data collator.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b8622f4-98c4-45f2-a2ef-bf2943ef2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid non ha lo shuffling\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "def1c79c-2a4b-4564-8ebc-f665ee7c93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: labels v.shape: torch.Size([32])\n",
      "k: input_ids v.shape: torch.Size([32, 34])\n",
      "k: attention_mask v.shape: torch.Size([32, 34])\n"
     ]
    }
   ],
   "source": [
    "# Vediamo come funziona:\n",
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(\"k:\", k, \"v.shape:\", v.shape)\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "Esempio output da analizzare:\n",
    "k: labels v.shape: torch.Size([32]) ---> 32 è la batch size, sono le label perchè abbiamo usato 32 di batch\n",
    "k: input_ids v.shape: torch.Size([32, 51]) ----> 51, invece, è la dimensione T. In questo caso abbiamo una sequenza di 51 con il collator\n",
    "k: attention_mask v.shape: torch.Size([32, 51]) ---> La mask si porta appresso la stessa dim\n",
    "\"\"\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d5537e0-5c4e-4349-a138-348b1fb86750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vediamo quali sono i valori delle labels (num_classes)\n",
    "set(tokenized_datasets[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b4f149e-d734-41ea-82b9-cc0ff48e89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check per la vocabulary size:\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab860622-9193-4253-9c43-4f92396123a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check max sequence lenght per il tokenizer:\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323dfc53-4c10-44f1-91df-9f87886aa44c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3183feaa-7c7c-473d-a66b-2d4d1171f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ricordiamo: ci sono alcune cose che vengono da costruzione del tokenizer, altre scelte arbitrariamente\n",
    "model = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=tokenizer.model_max_length,\n",
    "    d_k=16,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    n_classes=2, # Numero delle label che abbiamo nel dataset\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9f4b2-5e55-462c-8048-40b181b48516",
   "metadata": {},
   "source": [
    "### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6afe86c8-91d9-4226-8c83-3651c669101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce5e65-13b0-4475-8162-132607546133",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c720d7e9-5394-4cdf-bc5c-015d4af99670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd1ce4e7-5058-4f03-9b77-6328b5e1670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    valid_losses = np.zeros(epochs)\n",
    "\n",
    "    for it in range(epochs):\n",
    "        # SETTIAMO IN TRAIN MODE\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = 0\n",
    "        n_train = 0 # numero di campioni di training\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # muoviamo su GPU i dati\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # azzeriamo i gradienti (evita di accumulare il gradiente da iterazioni precedenti)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass + loss\n",
    "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            loss = criterion(outputs, batch[\"labels\"])\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # train loss computing:\n",
    "            # la loss function di pytorch è una media su tutti i campioni di un batch.\n",
    "            # per ottenere la loss su tutto il train set nella epoch, dobbiamo moltiplicare per il numero di campioni in un batch e sommare su tutto\n",
    "            # infine teniamo traccia di quanti campioni abbiamo per dividere\n",
    "            bs = batch[\"input_ids\"].size(0)\n",
    "            train_loss += loss.item() * bs\n",
    "            n_train += bs\n",
    "\n",
    "        train_loss = train_loss / n_train\n",
    "\n",
    "        # SWITCH A VALIDATION MODE (non facciamo step di azzeramento gradienti e backpropagation, solo forward)\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        n_valid = 0\n",
    "\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            loss = criterion(outputs, batch[\"labels\"])\n",
    "            bs = batch[\"input_ids\"].size(0)\n",
    "            valid_loss += loss.item() * bs\n",
    "            n_valid += bs\n",
    "        valid_loss = valid_loss / n_valid\n",
    "\n",
    "        train_losses[it] = train_loss\n",
    "        valid_losses[it] = valid_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f\"Epoch: {it+1}/{epochs} --- Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {dt}\")\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3435c047-968f-47af-b5e3-ecb16e20c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 --- Train Loss: 0.5341, Valid Loss: 0.5090, Duration: 0:00:17.210935\n",
      "Epoch: 2/4 --- Train Loss: 0.3684, Valid Loss: 0.4825, Duration: 0:00:16.586278\n",
      "Epoch: 3/4 --- Train Loss: 0.2991, Valid Loss: 0.4938, Duration: 0:00:16.416945\n",
      "Epoch: 4/4 --- Train Loss: 0.2606, Valid Loss: 0.5089, Duration: 0:00:17.159260\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train(model, criterion, optimizer, train_loader, valid_loader, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735da8a-5125-4b2a-bcc3-8f37a5050bbe",
   "metadata": {},
   "source": [
    "### Testiamo l'accuracy del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f141f68c-da69-47dd-998b-86ff2a62e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a823aa-5169-42b3-a3d2-fd399f37ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9313575554202735 f1: 0.9374092437783264\n",
      "Validation Accuracy: 0.786697247706422 f1: 0.7917288733166009\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "f1 = 0\n",
    "for batch in train_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    _, predictions = torch.max(outputs, 1) # torch.max ci torna il valore e l'indice (che ci serve). 1 è la dimensione sul quale calcoliamo il max\n",
    "    partial_f1 = f1_score(batch[\"labels\"].cpu().numpy(), predictions.cpu().numpy())\n",
    "    n_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "    n_total += batch[\"labels\"].shape[0]\n",
    "    f1 += partial_f1 * batch[\"labels\"].shape[0]\n",
    "\n",
    "train_acc = n_correct / n_total\n",
    "f1 = f1 / n_total\n",
    "print(\"Training Accuracy:\", train_acc, \"f1:\", f1)\n",
    "\n",
    "n_correct = 0\n",
    "n_total = 0\n",
    "f1 = 0\n",
    "for batch in valid_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    _, predictions = torch.max(outputs, 1) # torch.max ci torna il valore e l'indice (che ci serve). 1 è la dimensione sul quale calcoliamo il max\n",
    "    n_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "    partial_f1 = f1_score(batch[\"labels\"].cpu().numpy(), predictions.cpu().numpy())\n",
    "    n_total += batch[\"labels\"].shape[0]\n",
    "    f1 += partial_f1 * batch[\"labels\"].shape[0]\n",
    "\n",
    "valid_acc = n_correct / n_total\n",
    "f1 = f1 / n_total\n",
    "print(\"Validation Accuracy:\", valid_acc,  \"f1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f076b6b-b1a7-4aec-8ef3-122a4271f212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
