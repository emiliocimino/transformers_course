{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b680561f-f8c0-4ce6-962a-ffd5f3cb5969",
   "metadata": {},
   "source": [
    "# Seq2Seq Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8f043-678c-44e4-b1ba-1b71d859903f",
   "metadata": {},
   "source": [
    "## Class Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ce4db9-5e0d-4031-a0b7-2429ba076a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9734bf7c-fe27-466f-9ea6-a36162dcc4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        # Come il decoder se è causal, altrimenti no\n",
    "        if causal:\n",
    "            cm = torch.tril(torch.ones(max_len, max_len))\n",
    "            self.register_buffer(\"causal_mask\", cm.view(1, 1, max_len, max_len))\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "\n",
    "        N = q.shape[0]\n",
    "        # Qui le cose cambiano un po': in un'architettura encoder-decoder, ad un certo punto ci sarò l'unione dell'encoder e del decoder e dovremo\n",
    "        # modellare quali output del decoder devono prestare attenzione a determianati input dell'encoder\n",
    "        # Nell'attention head del decoder, in particolare, arriveranno K e V da parte dell'encoder che si mixano con Q della causal attention\n",
    "        # del decoder stesso. Per questo motivo, siccome non possiamo garantire che la lunghezza dell'input dell'encoder e quello del decoder \n",
    "        # siano uguali, dobbiamo definire l'output shape dell'encoder e del decoder separatamente.\n",
    "        T_output = q.shape[1] # Lunghezza output del decoder\n",
    "        T_input = k.shape[1] # Lunghezza input dell'enocoder (anche v andava bene)\n",
    "\n",
    "        # Qui si riflettono i cambiamenti\n",
    "        q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if pad_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(pad_mask[:, None, None, :] == 0, float(\"-inf\"))\n",
    "        if self.causal:\n",
    "            # Qui cambia anche la causal mask: ci sarà t_input e t_output (dimensione matrice cross attention)\n",
    "            attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T_output, :T_input] == 0, float(\"-inf\"))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        A = attn_weights @ v\n",
    "        A = A.transpose(1, 2)\n",
    "        # Qui la dimensione di uscita sarà sempre T_output\n",
    "        A = A.contiguous().view(N, T_output, self.d_k * self.n_heads)\n",
    "        \n",
    "        return self.fc(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c640408-abc3-4214-ae1d-8b9c5346815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.ln1(x + self.mha(x, x, x, mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07908418-6ecf-4338-aa5d-2eaf9dff1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "        self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "       \n",
    "    \n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        # In primo luogo, prendiamo l'input del decoder e lo passiamo nella self attention del decoder\n",
    "        x = self.ln1(dec_input + self.mha1(dec_input, dec_input, dec_input, dec_mask))\n",
    "\n",
    "        # Ora applichiamo la cross attention\n",
    "        x = self.ln2(x + self.mha1(x, enc_output, enc_output, enc_mask))\n",
    "\n",
    "        # Infine FC e dropout\n",
    "        x = self.ln3(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab20b3ce-7640-427c-9d93-b66e86a30edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        div_term = torch.exp(exp_term * (- math.log(10000) / d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0,:,0::2] = torch.sin(position * div_term) \n",
    "        pe[0,:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :] \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0d8a92-4585-48be-9e4d-526cb7958ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questa classe subisce diverse modifiche. In primo luogo, siccome non stiamo solo codificando, le n_classes non ci servono più\n",
    "Questo elimina di conseguenza anche la testa del modello, ovvero il classificatore.\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [EncoderBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformers_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformers_blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Altra modifica importante: qui ci serviranno tutti gli output, quindi non prendiamo più solo il primo \"hidden layer\"\n",
    "        x = self.ln(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486c4b87-84f6-4fba-b984-9b02b8a4d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [DecoderBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size) # Qui cambia rispetto all'encoder! Abbiamo bisogno della vocab size\n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        x = self.embedding(dec_input)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(enc_output, x, enc_mask, dec_mask)\n",
    "            \n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x) # many to many\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5efbc-b503-452c-9c51-aa9a79f1a913",
   "metadata": {},
   "source": [
    "#### Reminder di come funziona un Transformer Seq2Seq:\n",
    "\n",
    "In primo luogo l'input passa per tutti gli EncoderBlocks. Quindi avremo il processing dell'encoder che produrrà dei vettori key, query e value,\n",
    "ma solo dell'ultimo encoder!\n",
    "\n",
    "Questi t_input vettori non vengono usati tutti. Solo K e V passano al decoder (come encoder_output), e questi vengono usati solamente nella mha2. Tuttavia, a differenza dell'encoder che lavora sempre \"con i suoi input\", il decoder prenderà in ingresso l'output della trafila dell'encoder autonomamente. Quindi l'output dell'encoder entra in ingresso ad ogni \"DecoderBlock\" del decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd978285-a09c-42b6-959b-06adec50bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "        enc_output = encoder(enc_input, enc_mask)\n",
    "        dec_output = decoder (enc_output, dec_input, enc_mask, dec_mask)\n",
    "        return dec_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895f42a-903f-46a1-a67f-1c18b73b6e8f",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730d0998-3ae9-474d-b993-1fdbb88a74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=20000, \n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64, \n",
    "    n_layers=4,\n",
    "    n_heads=2,\n",
    "    dropout_prob=0.1             \n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=10000, \n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64, \n",
    "    n_layers=4,\n",
    "    n_heads=2,\n",
    "    dropout_prob=0.1  \n",
    ")\n",
    "model = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aff8ba6-cd68-4ea0-a98e-789dfbe4b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c756f266-6ada-4a94-a80f-34fc61f96e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.to(device);\n",
    "decoder.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830199b3-996c-4325-8ce2-5253b9f86b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo l'input dell'encoder randomicamente, con T = 512\n",
    "xe = np.random.randint(0, 20000, size=(8, 512))\n",
    "xe_t = torch.tensor(xe).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ae42c1f-1fcf-462b-98c4-58c5fef3e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora creiamo l'input del decoder randomicamente, ma lui con T = 256\n",
    "xd = np.random.randint(0, 10000, size=(8, 256))\n",
    "xd_t = torch.tensor(xd).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ee0c18-d385-44bb-a68d-cc05d5143992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo le relative maschere\n",
    "maske = np.ones((8, 512))\n",
    "maske[:, 256:] = 0\n",
    "maske_t = torch.tensor(maske).to(device)\n",
    "\n",
    "maskd = np.ones((8, 256))\n",
    "maskd[:, 128:] = 0\n",
    "maskd_t = torch.tensor(maskd).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab331000-823c-43c9-aeb5-2510212550b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(xe_t, xd_t, maske_t, maskd_t)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df8194-2df5-488e-ac61-91c5e2e054dc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "926debad-f2a6-446c-aa19-43af071cd7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5724bd68-c5b7-427b-87ae-9f0df85a1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"spa.txt\"\n",
    "if not os.path.isfile(fpath):\n",
    "    !curl -o \"spa.txt\" \"https://lazyprogrammer.me/course_files/nlp3/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c57de4-9696-4730-a17f-efff60ddf7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1\n",
       "0   Go.      Ve.\n",
       "1   Go.    Vete.\n",
       "2   Go.    Vaya.\n",
       "3   Hi.    Hola.\n",
       "4  Run!  ¡Corre!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eng to spanish translation\n",
    "df = pd.read_csv(\"spa.txt\", sep=\"\\t\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9a7cafe-f381-42ac-9a5e-695f32d76d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(30000).reset_index(drop=True) # Accorciamo o non finisce mai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d71685-bf72-456b-bc0f-95597cc985fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It wasn't premeditated.</td>\n",
       "      <td>No fue premeditado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is thinking about applying for a better-pa...</td>\n",
       "      <td>Tom está pensando en postular a un trabajo que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We'll talk about that later.</td>\n",
       "      <td>Conversaremos sobre eso después.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think Tom is devious.</td>\n",
       "      <td>Creo que Tom es astuto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They've fired him.</td>\n",
       "      <td>Lo despidieron.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                            It wasn't premeditated.   \n",
       "1  Tom is thinking about applying for a better-pa...   \n",
       "2                       We'll talk about that later.   \n",
       "3                            I think Tom is devious.   \n",
       "4                                 They've fired him.   \n",
       "\n",
       "                                                   1  \n",
       "0                                No fue premeditado.  \n",
       "1  Tom está pensando en postular a un trabajo que...  \n",
       "2                   Conversaremos sobre eso después.  \n",
       "3                            Creo que Tom es astuto.  \n",
       "4                                    Lo despidieron.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f04f091-1f3d-4e89-bb4c-406a583739c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"en\", \"es\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf02b98-4261-436d-abce-c0fcf1ae77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"spa.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c1ca2e0-cde3-41b0-a14a-456e5cdfe7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 30000 examples [00:00, 529432.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"csv\", data_files=\"spa.csv\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a447bdea-7318-429b-8986-09fd6787e949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'es'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = raw_datasets[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
    "split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5ffa9-7560-476d-84c4-98a67a316315",
   "metadata": {},
   "source": [
    "## Tokenizer & Data Collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e7e9115-57f6-4e9a-9229-b1d0a40410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ec7f551-4ea5-47c7-a634-0b35ac5637e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92d9d016-4b17-4bff-90b1-3f3f83419a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁Es', '▁difícil', '▁creer', '▁lo', '▁que', '▁dices', '.', '</s>'],\n",
       " 'Es difícil creer lo que dices.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split[\"train\"][0][\"en\"]\n",
    "es_sentence = split[\"train\"][0][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "\n",
    "# NB: Ci manca il token di padding all'inizio per il target. Questo non va bene per decoder, risolveremo dopo\n",
    "tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]), es_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50784231-be3c-462c-b930-5b072297a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_lens = [len(t[\"en\"]) for t in split[\"train\"]]\n",
    "plt.hist(en_lens, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "408b6705-5704-4359-a77d-68c6e7fd4b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_lens = [len(t[\"es\"]) for t in split[\"train\"]]\n",
    "plt.hist(es_lens, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f60c2b1d-760d-420c-81b7-29a7a28f037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possiamo usare tranquillamente una sequence lenght di 128 per entrambi\n",
    "max_input_length = 128\n",
    "max_output_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8841ea63-3d79-4564-8411-2f34a25e129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(batch[\"en\"], max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(batch[\"es\"], max_length=max_output_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a2e31a0-24de-4f2e-9f26-b587136cd108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████| 21000/21000 [00:01<00:00, 15147.80 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 9000/9000 [00:00<00:00, 15996.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split.map(preprocess_function, batched=True, remove_columns=split[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91ab9cfc-9625-4da8-9f9d-1ce89a0a138d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba50c5d7-9613-4908-acd7-d42bfa1db2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48dcb93d-19ce-4ce4-9a2f-f39035ebd913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c11384-b5d3-45e9-899e-ae4e8b653b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   94,    31,  2445,    13,  1232,   150,    40,   560,     3,     0,\n",
       "         65000, 65000, 65000],\n",
       "        [ 2760,   138,  4289,     5,   398,    53, 17756,     3,     0, 65000,\n",
       "         65000, 65000, 65000],\n",
       "        [   33,   325,    13,   634,   120,   174,  5266,    59,    73,  1358,\n",
       "          1101,     3,     0],\n",
       "        [  124,   172,  9103,    16,     5, 11367,     3,     0, 65000, 65000,\n",
       "         65000, 65000, 65000],\n",
       "        [ 1675,    40,   105,     7,  5266,    20,     9, 14506,    21,     0,\n",
       "         65000, 65000, 65000]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB: pad tokens non sono 0 ma 65k\n",
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc6fbb43-4feb-4471-86e8-9bdf6310c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Riprova che 65k è il pad\n",
    "batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a304162-735a-4215-878c-14cb5b40d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  177,  1162,   698,  1560,  9731,  8770,   237,    57,    15,    25,\n",
       "         15849,     9,     3,     0,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [   69, 13584,   203,    15,    15,  9763, 17109,     6,    25,  2785,\n",
       "            43,  8753,  4862,  1754,     3,     0,  -100,  -100,  -100,  -100],\n",
       "        [ 7511,  2367,   730, 36009,  4565,    91,   122,  5266,    87,    25,\n",
       "          3883,  6235,   151,     8,     6,    25, 36009,  7203,     3,     0],\n",
       "        [   71,     9, 12174,  4190,  4564,  7697,    12,    14,    25,  6574,\n",
       "           487,  4683,     3,     0,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [   25,  7287,   490,   100,     9,    28,    97,     4,    17,  2564,\n",
       "          6323,     9,     4,  5266,    21,     0,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Il pad del target è diverso!! E' -100\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5be5782a-677e-41f2-9a49-ef3325c5ec84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 65000]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 -> End of sentence, 65k -> PAD, 1 -> ??\n",
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e472addb-d17f-46bf-b9a7-003ec405fcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<unk>', '<pad>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 corrisponde all'unkown token!!\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c50440a1-aea2-4158-8352-5bc7e0c28fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [65000, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un tokenizer mette sempre uno 0 a qualsiasi cosa passiamo. Questo ci potrà essere sia utile che di intralcio\n",
    "# Facciamo un esempio passando un pad\n",
    "tokenizer(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011f2d5-3857-4725-9403-1b76fd41860c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438bcd6-775e-4a2c-ba5d-9ebea20a6c96",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "572b362f-671f-47b9-a41f-e06010ae1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38bf7da6-2eca-4735-af8b-c2660fa2891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e31d1fbb-4628-42b2-94ad-40b403641ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "140f29c6-af66-4255-9dd8-55655711bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: input_ids value shape: torch.Size([32, 14])\n",
      "key: attention_mask value shape: torch.Size([32, 14])\n",
      "key: labels value shape: torch.Size([32, 25])\n"
     ]
    }
   ],
   "source": [
    "# Vediamo cosa fa\n",
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(\"key:\", k, \"value shape:\", v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18e780a0-e644-4c3e-853e-005be6a7edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ee1a7ef-a3ae-440f-8d13-f020f6dc98b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Il tokenizer, tuttavia, sembra non avere il token \"start of sentence\": <s>, quindi glielo mettiamo noi\n",
    "# Stiamo praticamente creando il CLS token di bert, usando <s> come token\n",
    "# Come sappiamo che manca il CLS? Beh, abbiamo esplorato prima gli special token e vi era solo end of sentence, unknown e padding\n",
    "# possiamo usare \n",
    "tokenizer.add_special_tokens({\"cls_token\": \"<s>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b42dc7e-dfab-4bc5-88db-a71491ada97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [65001, 0], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89bce2dd-ff42-43cc-8e0e-03bca76d46bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65001"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nonostante l'operazione, pare che la vocab size non sia cambiata, quindi dovremmo aggiornarla noi manualmente\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38817ad7-c4ee-47e1-af0d-63d5ac06e652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 65000, 65001]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Però adesso notiamo che effettivamente abbiamo uno special id.\n",
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f104b3-b894-4b5e-b8b3-6762cb29a983",
   "metadata": {},
   "source": [
    "### Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cf10073-f5f8-4aed-b4b4-d4b4f8cf7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size+1, \n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64, \n",
    "    n_layers=4,\n",
    "    n_heads=2,\n",
    "    dropout_prob=0.1             \n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=tokenizer.vocab_size+1, \n",
    "    max_len=512,\n",
    "    d_k=16,\n",
    "    d_model=64, \n",
    "    n_layers=4,\n",
    "    n_heads=2,\n",
    "    dropout_prob=0.1  \n",
    ")\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb575a39-f122-489f-9f0f-cef476e3bb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(65002, 64)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformers_blocks): Sequential(\n",
       "      (0): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(65002, 64)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): Sequential(\n",
       "      (0): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha1): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiHeadAttention(\n",
       "          (key): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (query): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "        )\n",
       "        (ann): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=64, out_features=65002, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perchè nel corso non mette direttamente il transformer nel device?\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6e8bb-1374-4bc3-aad8-2ca7ea99216a",
   "metadata": {},
   "source": [
    "### Loss e Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57542c49-8081-454a-8c8a-0d5d0832575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qui gli diciamo quale criterio usiamo per la loss. Diciamo di ignorare l'indice -100 che è quello del padding\n",
    "# ATTENZIONE. Il -100 è quello delle labels! Noi è sulle label che vogliamo ignorare\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cb301-bdf8-491a-a1d1-7bfd44233fa2",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8ce9182-52a8-4c28-a5eb-68a48b8fd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bde5f55a-1446-44f5-848d-c00d2efa2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(data_loader, model, criterion, optimizer):\n",
    "    batch_loss = []\n",
    "    for batch in data_loader:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        enc_inputs = batch[\"input_ids\"]\n",
    "        enc_mask = batch[\"attention_mask\"]\n",
    "        targets = batch[\"labels\"]\n",
    "\n",
    "        # Shiftiamo i target avanti per avere l'input del decoder. E' come se pushassimo le parole verso destra\n",
    "        # Infine, inseriamo a prima posizione il nostro \"start_token\"\n",
    "        dec_inputs = targets.clone().detach()\n",
    "        dec_inputs = torch.roll(dec_inputs, shifts=1, dims=1)\n",
    "        dec_inputs[:, 0] = 65001\n",
    "\n",
    "        # Convertiamo tutti i pad token che sono -100 in pad token di tokenizer, ovvero 65k)\n",
    "        dec_inputs = dec_inputs.masked_fill(dec_inputs==-100, tokenizer.pad_token_id)\n",
    "\n",
    "        # Creiamo la decoder mask\n",
    "        dec_mask = torch.ones_like(dec_inputs)\n",
    "        dec_mask = dec_mask.masked_fill(dec_inputs==tokenizer.pad_token_id, 0)\n",
    "\n",
    "        # Facciamo il forward pass\n",
    "        outputs = model(enc_inputs, dec_inputs, enc_mask, dec_mask)\n",
    "        # Ricordiamo questo passaggio: lui ritornerà una determianta dimensione (Batch x T x Vocab_size) ma a noi serve avere la vocab size come\n",
    "        # seconda dimensione, quindi lo trasponiamo\n",
    "        loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.item())\n",
    "    return np.mean(batch_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "686bbe69-4533-4125-b1d6-778258cecfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = np.zeros(n_epochs)\n",
    "    valid_losses = np.zeros(n_epochs)\n",
    "\n",
    "    for it in range(n_epochs):\n",
    "        model.train()\n",
    "        d0 = datetime.now()\n",
    "        train_loss = training_step(train_loader, model, criterion, optimizer)\n",
    "        train_losses[it] = train_loss\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = training_step(valid_loader, model, criterion, optimizer)\n",
    "        valid_losses[it] = valid_loss\n",
    "\n",
    "        dt = datetime.now() - d0\n",
    "        print(f\"Epoch: {it+1}/{n_epochs} --- Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {dt}\")\n",
    "    return train_losses, valid_losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d407a53b-5f20-4c7c-86d4-122dccf2d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = np.zeros(n_epochs)\n",
    "    valid_losses = np.zeros(n_epochs)\n",
    "\n",
    "    for it in range(n_epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        batch_train_loss = []\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            enc_inputs = batch[\"input_ids\"]\n",
    "            enc_mask = batch[\"attention_mask\"]\n",
    "            targets = batch[\"labels\"]\n",
    "\n",
    "            # Shiftiamo i target avanti per avere l'input del decoder. E' come se pushassimo le parole verso destra\n",
    "            # Infine, inseriamo a prima posizione il nostro \"start_token\"\n",
    "            dec_inputs = targets.clone().detach()\n",
    "            dec_inputs = torch.roll(dec_inputs, shifts=1, dims=1)\n",
    "            dec_inputs[:, 0] = 65001\n",
    "\n",
    "            # Convertiamo tutti i pad token che sono -100 in pad token di tokenizer, ovvero 65k)\n",
    "            dec_inputs = dec_inputs.masked_fill(dec_inputs==-100, tokenizer.pad_token_id)\n",
    "\n",
    "            # Creiamo la decoder mask\n",
    "            dec_mask = torch.ones_like(dec_inputs)\n",
    "            dec_mask = dec_mask.masked_fill(dec_inputs==tokenizer.pad_token_id, 0)\n",
    "\n",
    "            # Facciamo il forward pass\n",
    "            outputs = model(enc_inputs, dec_inputs, enc_mask, dec_mask)\n",
    "            # Ricordiamo questo passaggio: lui ritornerà una determianta dimensione (Batch x T x Vocab_size) ma a noi serve avere la vocab size come\n",
    "            # seconda dimensione, quindi lo trasponiamo\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_loss.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(batch_train_loss)\n",
    "        train_losses[it] = train_loss\n",
    "\n",
    "        model.eval()\n",
    "        batch_valid_loss = []\n",
    "        for batch in valid_loader:\n",
    "            batch = {k: v.to(device) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            enc_inputs = batch[\"input_ids\"]\n",
    "            enc_mask = batch[\"attention_mask\"]\n",
    "            targets = batch[\"labels\"]\n",
    "\n",
    "            # Shiftiamo i target avanti per avere l'input del decoder. E' come se pushassimo le parole verso destra\n",
    "            # Infine, inseriamo a prima posizione il nostro \"start_token\"\n",
    "            dec_inputs = targets.clone().detach()\n",
    "            dec_inputs = torch.roll(dec_inputs, shifts=1, dims=1)\n",
    "            dec_inputs[:, 0] = 65001\n",
    "\n",
    "            # Convertiamo tutti i pad token che sono -100 in pad token di tokenizer, ovvero 65k)\n",
    "            dec_inputs = dec_inputs.masked_fill(dec_inputs==-100, tokenizer.pad_token_id)\n",
    "\n",
    "            # Creiamo la decoder mask\n",
    "            dec_mask = torch.ones_like(dec_inputs)\n",
    "            dec_mask = dec_mask.masked_fill(dec_inputs==tokenizer.pad_token_id, 0)\n",
    "\n",
    "            # Facciamo il forward pass\n",
    "            outputs = model(enc_inputs, dec_inputs, enc_mask, dec_mask)\n",
    "            # Ricordiamo questo passaggio: lui ritornerà una determianta dimensione (Batch x T x Vocab_size) ma a noi serve avere la vocab size come\n",
    "            # seconda dimensione, quindi lo trasponiamo\n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_valid_loss.append(loss.item())\n",
    "        \n",
    "        valid_loss = np.mean(batch_valid_loss)\n",
    "        valid_losses[it] = valid_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f\"Epoch: {it+1}/{n_epochs} --- Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {dt}\")\n",
    "    return train_losses, valid_losses     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1757d0d-928e-40bd-bad5-2454462c79e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 --- Train Loss: 3.7290, Valid Loss: 3.1734, Duration: 0:08:07.505014\n",
      "Epoch: 2/15 --- Train Loss: 3.2574, Valid Loss: 2.7322, Duration: 0:08:10.575956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 37\u001b[0m     batch_train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     39\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(batch_train_loss)\n\u001b[0;32m     40\u001b[0m train_losses[it] \u001b[38;5;241m=\u001b[39m train_loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = train(transformer, criterion, optimizer, train_loader, valid_loader, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea12ca-f99f-4ac5-9880-f0e59a5e706f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Per usare correttamente il modello, noi useremo encoder e decoder in maniera separata. Vorremmo calcolare prima il risultato dell'encoder, ovvero la codifica embedded della frase in inglese. Questa verrà successivamente inserita nel decoder per fare inferenza autoregressiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c0e1ea81-970d-41ad-81a5-a5504c0f37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "992226b3-fd59-4e9c-ad01-d4afec8de120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mary doesn't use salt in her cooking.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 11\n",
    "input_sentence = split[\"test\"][sample][\"en\"]\n",
    "input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a2f686d4-e669-4d18-8bb1-164605f25baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4963,  2088,    20,    56,   268, 11626,    16,   225, 15578,     3,\n",
       "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c12854d2-5087-4b86-b331-4f94034655ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[65001,     0]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serve l'input del decoder. Siccome useremo il decoder in modalità generativa (inferene), noi avremo solamente l'input <s>\n",
    "# ignoreremo il secondo token\n",
    "dec_input_str = \"<s>\"\n",
    "dec_input = tokenizer(text_target=dec_input_str, return_tensors=\"pt\")\n",
    "dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "151d0b53-ab27-46e9-bf5e-91d4ffb18daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[65001,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input.to(device)\n",
    "dec_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fcd17680-962a-4c51-bd6d-5a5e58987c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2466, -4.5174,  2.2848,  ..., -6.8810, -7.8362, -7.3809]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = transformer(\n",
    "    enc_input[\"input_ids\"],\n",
    "    dec_input[\"input_ids\"][:,:-1],\n",
    "    enc_input[\"attention_mask\"],\n",
    "    dec_input[\"attention_mask\"][:,:-1]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "94058f34-2972-43b8-b811-3001680be62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 65002])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ad06541-7883-4793-a8f6-431c7381ce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 64])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output = encoder(enc_input[\"input_ids\"], enc_input[\"attention_mask\"])\n",
    "enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "87ff4b5b-1dfa-48dd-8ef3-558a762b0f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2466, -4.5174,  2.2848,  ..., -6.8810, -7.8362, -7.3809]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output = decoder(\n",
    "    enc_output,\n",
    "    dec_input[\"input_ids\"][:,:-1],\n",
    "    enc_input[\"attention_mask\"],\n",
    "    dec_input[\"attention_mask\"][:,:-1]\n",
    ")\n",
    "dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "950fa156-306a-4264-b5a6-64c24690feba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 65002])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "826ffeee-6118-4417-ac7a-282c4bc0f5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Questo è un sanity check: se usiamo prima encoder e decoder e poi usiamo l'intero transformer, abbiamo lo stesso risultato?\n",
    "torch.allclose(output, dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02644c6f-39a1-4342-a32e-0f3d459f0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Autoregressivo\n",
    "dec_input_ids = dec_input[\"input_ids\"][:,:-1]\n",
    "dec_attn_mask = dec_input[\"attention_mask\"][:,:-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    dec_output = decoder(\n",
    "        enc_output,\n",
    "        dec_input_ids,\n",
    "        enc_input[\"attention_mask\"],\n",
    "        dec_attn_mask\n",
    "    )\n",
    "\n",
    "    # Prendiamo il token più probabile (o campioniamo con softmax) sull'ultimo token del decoder (ultimo timestep), maxando sulla dimensione vocab\n",
    "    prediction_id = torch.argmax(dec_output[:,-1,:], axis=-1)\n",
    "\n",
    "    # Appendiamo il token predetto e aggiorniamo la mask del decoder\n",
    "    # print(prediction_id.shape) -> questo ci da un tensore di size (1, ). A noi serve bidimensionale\n",
    "    dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1,1)))\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "    # Se troviamo il </s> finale, interrompiamo\n",
    "    if prediction_id == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f5b233f8-2f23-43e5-b4cc-9a470b92add4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65001,  4963,    37,    26,    25,   210, 14943,    12,    45,    25,\n",
       "           210, 10031,     3,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1af326fc-93af-43b6-b0e4-16d34dae6c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Mary no se llega en su llave.</s>'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dec_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "257cea4b-6ead-4b3f-9225-b1383754425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary cocina sin sal.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_real = split[\"test\"][sample][\"es\"]\n",
    "output_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3256b587-beae-4aa7-a42b-334afea01583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence):\n",
    "    \n",
    "    enc_input = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    enc_output = encoder(enc_input[\"input_ids\"], enc_input[\"attention_mask\"])\n",
    "\n",
    "    dec_input_str = \"<s>\"\n",
    "    dec_input = tokenizer(text_target=dec_input_str, return_tensors=\"pt\").to(device)\n",
    "    dec_input_ids = dec_input[\"input_ids\"][:,:-1]\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "    for _ in range(100):\n",
    "        dec_output = decoder(\n",
    "            enc_output,\n",
    "            dec_input_ids,\n",
    "            enc_input[\"attention_mask\"],\n",
    "            dec_attn_mask\n",
    "        )\n",
    "    \n",
    "        # Prendiamo il token più probabile (o campioniamo con softmax) sull'ultimo token del decoder (ultimo timestep), maxando sulla dimensione vocab\n",
    "        prediction_id = torch.argmax(dec_output[:,-1,:], axis=-1)\n",
    "    \n",
    "        # Appendiamo il token predetto e aggiorniamo la mask del decoder\n",
    "        # print(prediction_id.shape) -> questo ci da un tensore di size (1, ). A noi serve bidimensionale\n",
    "        dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1,1)))\n",
    "        dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "    \n",
    "        # Se troviamo il </s> finale, interrompiamo\n",
    "        if prediction_id == 0:\n",
    "            break\n",
    "            \n",
    "    print(tokenizer.decode(dec_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d2bf011e-58ff-4da1-94f8-5d13bd38c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Si te qué es más, no es más, no es más.</s>\n"
     ]
    }
   ],
   "source": [
    "translate(\"If you think you are correct, do so, and you will never be wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bbde2-57d9-44e9-a768-f0f1fe27673a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d290a8-f23d-4708-bf46-0dc2d0072bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
