{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d822901-29d9-4854-ac56-3834af3242c4",
   "metadata": {},
   "source": [
    "# Parts of Speech Exercise\n",
    "\n",
    "1) Rivedi la lezione di \"start\" dell'esercizio, dopo di che prova ad applicare le logiche del NER al POS\n",
    "2) Scaricare il brown corpus e la lista universale dei tags\n",
    "3) Creare un file JSON da cui caricare i dati\n",
    "     - EXAMPLE {\"inputs\": [\"Cats\", \"are\", \"animals\"], \"tags\":[6,2,4]}\n",
    "     - Usare load_dataset(\"json\", data_files=\"data.json\") per importare\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97747c-62fb-40b0-bf08-f63ba3916221",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f347d2-6e28-41b4-9845-ceadd25e99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915f31a3-6ba5-4a90-b5a8-770f95aebf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\emili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\emili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"universal_tagset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77660c28-362b-4f3e-bfb5-bb4a98969e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af65a9d-11b4-4504-b9b0-43a5bde6c78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of lists\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e84b382-62aa-4d8d-bf0a-0d575896180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb5a2b2-ca6a-4cbc-90f8-4712a91c981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags(corpus):\n",
    "    list_tags = []\n",
    "    for sentence in corpus:\n",
    "        for _, tag in sentence:\n",
    "            list_tags.append(tag)\n",
    "\n",
    "    return set(list_tags)\n",
    "\n",
    "tag_names = extract_tags(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7f7da6-7e1d-48e3-85fe-bc660a03192e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3589e1-86d0-4899-a361-26c063c86edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {k:v for k,v in enumerate(tag_names)}\n",
    "tag2id = {v:k for k,v in id2tag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8dc21f-1216-4964-848b-7d763943e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VERB': 0,\n",
       " 'NOUN': 1,\n",
       " 'ADJ': 2,\n",
       " 'ADP': 3,\n",
       " 'ADV': 4,\n",
       " 'CONJ': 5,\n",
       " '.': 6,\n",
       " 'NUM': 7,\n",
       " 'DET': 8,\n",
       " 'PRT': 9,\n",
       " 'X': 10,\n",
       " 'PRON': 11}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5525e67e-3f4b-40e2-ba86-6a9bae758c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"data/brown.json\"\n",
    "def create_json_from_corpus(corpus):\n",
    "    list_of_inputs = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        curr_object = {\"inputs\": [], \"tags\": []}\n",
    "        for inp, tag in sentence:\n",
    "            curr_object[\"inputs\"].append(inp)\n",
    "            curr_object[\"tags\"].append(tag2id[tag])\n",
    "        list_of_inputs.append(curr_object)\n",
    "\n",
    "    with open(fpath, \"w\") as f:\n",
    "        json.dump(list_of_inputs, f)\n",
    "\n",
    "if not os.path.isfile(fpath):\n",
    "    create_json_from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af4fff-317c-4b02-9141-e90bf50cdef4",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85a592f-15e5-4b1e-8427-6fe9c67181fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "checkpoint = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aacfb52-7bae-4342-8096-2a69f3f8b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 57340 examples [00:00, 146493.64 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'tags'],\n",
       "        num_rows: 57340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=fpath)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a295dcd5-7d1a-4ddd-9e20-024cf907f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], 'tags': [8, 1, 1, 2, 1, 0, 1, 8, 1, 3, 1, 2, 1, 1, 0, 6, 8, 1, 6, 3, 8, 1, 0, 1, 6]}\n"
     ]
    }
   ],
   "source": [
    "test_row = dataset[\"train\"][0]\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2bb43c-a9ab-4149-a4f3-4773889e4211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'tags'],\n",
       "        num_rows: 40138\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'tags'],\n",
       "        num_rows: 17202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add2b96b-a366-4600-ab4e-bcf4d8ec4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9084447-b004-49a8-9e96-f1e6a2aa8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'\", 's', 'recent', 'primary', 'election', 'produced', '`', '`', 'no', 'evidence', \"'\", \"'\", 'that', 'any', 'irregular', '##ities', 'took', 'place', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 11, 12, 13, 14, 15, 15, 16, 17, 18, 18, 19, 20, 21, 21, 22, 23, 24, None]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer(test_row[\"inputs\"], is_split_into_words=True)\n",
    "print(t.tokens())\n",
    "print(t.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11322f2f-046a-4707-8c8b-ef12de757346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tags(ids, tags):\n",
    "    aligned_targets = []\n",
    "    \n",
    "    for wid in ids:\n",
    "        if wid is None:\n",
    "            # Qui abbiamo un token speciale da ignorare (id: None)\n",
    "            label = -100\n",
    "        else:\n",
    "            label = tags[wid]\n",
    "\n",
    "        aligned_targets.append(label)\n",
    "            \n",
    "    return aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16608fc4-aab2-4d48-99b0-a7ab20b2e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(len(align_tags(t.word_ids(), test_row[\"tags\"])), len(t.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b54613a3-a502-4ee8-aa02-8f50c1d68aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"inputs\"], truncation=True, is_split_into_words=True)\n",
    "    labels_batch = batch[\"tags\"] # Original targets\n",
    "    aligned_batch_labels = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_id = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels = align_tags(word_id, labels)\n",
    "        aligned_batch_labels.append(aligned_labels)\n",
    "\n",
    "    # Ricordiamo: il nostro target DEVE essere salvato in una colonna chiamata \"labels\"\n",
    "    tokenized_inputs[\"labels\"] = aligned_batch_labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9496c28-3253-415b-ac8f-3a04f2c6899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████| 40138/40138 [00:03<00:00, 11351.34 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████| 17202/17202 [00:01<00:00, 13178.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split.map(tokenize_fn, batched=True, remove_columns=split[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82e58c90-404d-46a6-aea4-a4300e416cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40138\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398dafdc-56ba-4dda-a88e-cc59dfe3a35a",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c97503a-6e0a-41a3-a982-bf35623b13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7abe24c4-3b8c-489d-a06a-59bc0baf4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator =  DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0824235-62bd-40b9-83ba-0973e0ba312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    1,    1,    3,    1,    5,    1,    0,    4,    8,    2,    1,\n",
       "            3,    8,    1,    1,    1,    1,    6, -100],\n",
       "        [-100,    0,    1,    6,    6, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade6593-3210-401a-b36e-27770d29f909",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c22aa30f-d917-4ae4-a240-5184699162d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4461a170-2b26-447f-9cc2-9bae539ab89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    \n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Eliminiamo le predizioni e le labels dove non ci sono i -100\n",
    "    clean_labels = [t for label in labels for t in label if t!=-100]\n",
    "    clean_preds = [p for pred, targ in zip(predictions, labels) for p, t in zip(pred, targ) if t!=-100 ]\n",
    "\n",
    "    accuracy = accuracy_score(clean_labels, clean_preds)\n",
    "    f1 = f1_score(clean_labels, clean_preds, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec88093-d72a-4113-bed3-3956ca330715",
   "metadata": {},
   "source": [
    "## Model, Trainer, TrainerArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "259599b4-11e6-4c03-95b5-61041c424e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f62d8b3e-323b-4c83-a2b6-3fcd1bf7ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, id2label=id2tag, label2id=tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a379c96-47aa-4c92-b5b6-bbc6d92890c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"my_pos_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fceb4ed0-966a-4a70-91ed-b9b99cc2b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=train_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a33f08f-7dee-4fa2-87ec-6250aaab7914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25090' max='25090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25090/25090 35:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.986674</td>\n",
       "      <td>0.963302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.044557</td>\n",
       "      <td>0.987950</td>\n",
       "      <td>0.965363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>0.988402</td>\n",
       "      <td>0.968191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.053689</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>0.969614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.057711</td>\n",
       "      <td>0.988866</td>\n",
       "      <td>0.970157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25090, training_loss=0.03067116279343582, metrics={'train_runtime': 2104.6829, 'train_samples_per_second': 95.354, 'train_steps_per_second': 11.921, 'total_flos': 2588388393132576.0, 'train_loss': 0.03067116279343582, 'epoch': 5.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770677e-678f-44b7-8db0-d47e77910be9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5c4dd3-18e7-425a-a85b-9db3b95bb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\Desktop\\transformers_course\\transformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a079ef7c-28fa-4048-a6b6-2bbfba4103fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pipeline(\"token-classification\", model=\"my_pos_model/checkpoint-15054/\", aggregation_strategy=\"simple\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d68a3b2-9dd5-4fbc-917e-3697836c3102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'NOUN',\n",
       "  'score': 0.9998913,\n",
       "  'word': 'Snake',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': '.', 'score': 0.9999801, 'word': ',', 'start': 5, 'end': 6},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9998185,\n",
       "  'word': 'infiltrate',\n",
       "  'start': 7,\n",
       "  'end': 17},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.9999366,\n",
       "  'word': 'the',\n",
       "  'start': 18,\n",
       "  'end': 21},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99992555,\n",
       "  'word': 'enemy fortress',\n",
       "  'start': 22,\n",
       "  'end': 36},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9991744,\n",
       "  'word': 'Outer',\n",
       "  'start': 37,\n",
       "  'end': 42},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9998031,\n",
       "  'word': 'Heaven',\n",
       "  'start': 43,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos(\"Snake, infiltrate the enemy fortress Outer Heaven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5e7212-2d57-432e-acaf-f84629999a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PRON',\n",
       "  'score': 0.9998221,\n",
       "  'word': 'I',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9994816,\n",
       "  'word': 'am cooking',\n",
       "  'start': 2,\n",
       "  'end': 12},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99987876,\n",
       "  'word': 'a',\n",
       "  'start': 13,\n",
       "  'end': 14},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9994346,\n",
       "  'word': 'Genovese recipe',\n",
       "  'start': 15,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos(\"I am cooking a Genovese recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaf503-05f9-4dec-bf64-5fd87d60058e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "transformer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
